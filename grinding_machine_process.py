# -*- coding: utf-8 -*-
"""Grinding Machine Process.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hqlfi7rV8dpZoC1KkpCCWuGYYXA3eXVx
"""

!pip install deap

# Import libraries for data manipulation
import pandas as pd
import numpy as np
# Import libraries for data visualization
import matplotlib.pyplot as plt
import seaborn as sns
# Import libraries for data preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
# Import libraries for model building and evaluation
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import tensorflow as tf

from google.colab import files
uploaded = files.upload()

# Building a dataframe with the .csv
df = pd.read_csv('Grinding_Planilha.csv', sep=';', encoding= 'unicode_escape')
df.head()

#Excluding the first line of dataframe
df = df.drop(df.index[0])
#Excluding columnn 17
df = df.drop(df.columns[17], axis=1)
#Excluding lines with Status = NOK
df = df[df['Status'] != 'NOK']
#Excluding anothers Columns
df = df.drop(['Status', 'Sub_Parts_Status','Unnamed: 0', 'Station', 'Process','Quality Matrix Factor'], axis=1)

print(df.info())
print(df.describe())

# Converting all 'objects' features in 'numerics'
cols_to_convert = df.select_dtypes(include=['object']).columns
df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')

print(df.info())
print(df.describe())

# Showing the matrix correlations (Pearson, but could be Kendall or Spearman) setting threshold
plt.figure(figsize=(10,10))
sns.heatmap(df.corr(), cmap='turbo', annot=False)
plt.show()

# Showing the matrix correlations setting threshold
threshold = 0.5
corr_matrix = df.corr()
plt.figure(figsize=(10,10))
sns.heatmap(corr_matrix[abs(corr_matrix) >= threshold], cmap='turbo', annot=False)
plt.show()

"""**Algortimo Genético**
---


"""

#Analyze importance data trought random forest

#Spliting dataset in training and testing
sample_frac = 0.2  # Por exemplo, 20% da base
# Reduz a base mantendo o vínculo entre X e y
df_reduced = df.sample(frac=sample_frac, random_state=42)


X = df_reduced.drop(['Griding time'], axis=1)
y = df_reduced['Griding time']

scaler = MinMaxScaler()
X_scale = scaler.fit_transform(X)
X = pd.DataFrame(X_scale, columns=X.columns)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

from deap import base, creator, tools, algorithms


# === Setup ===
X = X_train  # ou seu DataFrame de treino
y = y_train
n_features = X.shape[1]

# === GA configuration ===
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
toolbox.register("attr_bool", random.randint, 0, 1)
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=n_features)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# === Função de fitness ===
def evalFeatureSelection(individual):
    if sum(individual) == 0:
        return -9999,  # evita solução com 0 features
    selected = [i for i, bit in enumerate(individual) if bit == 1]
    X_sel = X.iloc[:, selected]
    model = RandomForestRegressor()
    scores = cross_val_score(model, X_sel, y, cv=5, scoring='r2')
    return scores.mean(),

toolbox.register("evaluate", evalFeatureSelection)
toolbox.register("mate", tools.cxTwoPoint)
toolbox.register("mutate", tools.mutFlipBit, indpb=0.05)
toolbox.register("select", tools.selTournament, tournsize=3)

# === Execução e coleta de estatísticas ===
stats = tools.Statistics(lambda ind: ind.fitness.values)
stats.register("avg", np.mean)
stats.register("max", np.max)

pop = toolbox.population(n=50)
pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=20, stats=stats, verbose=True)

# === Plot: evolução do GA ===
gen = log.select("gen")
fit_max = log.select("max")
fit_avg = log.select("avg")

plt.figure(figsize=(8, 5))
plt.plot(gen, fit_max, label='Best Fitness', color = 'purple')
plt.plot(gen, fit_avg, label='Average Fitness', color = 'orange')
plt.xlabel("Generation")
plt.ylabel("Score (R²)")
plt.title("Genectic Algorithm Evolution")
plt.legend()
plt.tight_layout()
plt.show()

# === Melhor indivíduo ===
best_ind = tools.selBest(pop, 1)[0]
selected_features = [col for i, col in enumerate(X.columns) if best_ind[i] == 1]

print("Selected Features:", selected_features)

# === Treinar modelo final e plotar importância ===
X_sel_final = X[selected_features]
model_final = RandomForestRegressor()
model_final.fit(X_sel_final, y)

# Plot: Importância das features
importances = model_final.feature_importances_
df_imp = pd.DataFrame({
    'Feature': selected_features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=df_imp, color='purple')
plt.title('Importance of Features Selected by GA')
plt.tight_layout()
plt.show()

# === Predição e comparação com valores reais ===
y_pred = model_final.predict(X_sel_final)

plt.figure(figsize=(8, 6))
sns.scatterplot(x=y, y=y_pred)
plt.plot([y.min(), y.max()], [y.min(), y.max()], '--r')
plt.xlabel('Real Value')
plt.ylabel('Predicted Value')
plt.title('Prediction vs Real (Random Forest with Selected Features)')
plt.tight_layout()
plt.show()

"""**Análise Estatística**
---


"""

#Spliting dataset in training and testing
X = df.drop(['Griding time'], axis=1)
y = df['Griding time']

# Transformar o y em vetor numpy
y = np.array(y)

scaler = MinMaxScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

from re import sub
import matplotlib.pyplot as plt
import numpy as np

# Create the subplots
fig, axes = plt.subplots(4, 3, figsize=(15, 15))

axes[0,0].plot(X[:,1],y, 'o', color='chocolate')
axes[0,0].set_title('Initial Pressure vs Grinding Time')
axes[0,1].plot(X[:,2],y, 'o', color='chartreuse')
axes[0,1].set_title('Cut Off Flow Corrected vs Grinding Time')
axes[0,2].plot(X[:,4],y, 'o', color='aqua')
axes[0,2].set_title('Initial Flow Ratio Corrected vs Grinding Time')
axes[1,0].plot(X[:,5],y, 'o', color='red')
axes[1,0].set_title('Griding Temperature Oil vs Grinding Time')
axes[1,1].plot(X[:,12],y, 'o', color='darkviolet')
axes[1,1].set_title('Flow Rate Temperature Before Grinding vs Grinding Time')
axes[1,2].plot(X[:,7],y, 'o', color='black')
axes[1,2].set_title('Flow Pressure vs Grinding Time')
axes[2,0].plot(X[:,8],y, 'o', color='gold')
axes[2,0].set_title('Flow Corrected vs Grinding Time')
axes[2,1].plot(X[:,9],y, 'o', color='fuchsia')
axes[2,1].set_title('Oil Temperature vs Grinding Time')
axes[2,2].plot(X[:,11],y, 'o', color='lime')
axes[2,2].set_title('Flow Rate Before Grinding Corrected vs Grinding Time')
axes[3,0].plot(X[:,0],y, 'o', color='blue')
axes[3,0].set_title('Pressure 1 vs Grinding Time')
axes[3,1].plot(X[:,10],y, 'o', color='green')
axes[3,1].set_title('Flow Pressure Before Grinding vs Grinding Time')
axes[3,2].plot(X[:,6],y, 'o', color='orange')
axes[3,2].set_title('Rework Time vs Grinding Time')

plt.tight_layout()
plt.show

# Reduzindo Outliers
Q1 = df.quantile(0.15)
Q3 = df.quantile(0.95)
IQR = Q3 - Q1
upper_bound = Q3 + 1.75 * IQR
lower_bound = Q1 - 1.75 * IQR
outliers = df[(df > upper_bound)]

df_no_outliers = df[~((df > upper_bound) | (df < lower_bound)).any(axis=1)]
#df_no_outliers = df[~((df > upper_bound)).any(axis=1)]

'''
# Definindo os quartis e calculando o IQR para 'Flow Pressure'
Q1_fp = df_new['Flow Pressure'].quantile(0.55)
Q3_fp = df_new['Flow Pressure'].quantile(0.95)
IQR_fp = Q3_fp - Q1_fp
lower_bound_fp = Q1_fp - 1.5 * IQR_fp  # Limite inferior para outliers em 'Flow Pressure'

# Definindo os quartis e calculando o IQR para 'Flow Corrected'
Q1_fc = df_new['Flow Corrected'].quantile(0.35)
Q3_fc = df_new['Flow Corrected'].quantile(0.75)
IQR_fc = Q3_fc - Q1_fc
lower_bound_fc = Q1_fc - 1.5 * IQR_fc  # Limite inferior para outliers em 'Flow Corrected'

# Criando um DataFrame booleano para identificar outliers inferiores
outliers_fp = df_new['Flow Pressure'] < lower_bound_fp
outliers_fc = df_new['Flow Corrected'] < lower_bound_fc

# Combinando as condições para ambas as colunas usando any(axis=1)
outliers_combined = pd.DataFrame({'Flow Pressure': outliers_fp, 'Flow Corrected': outliers_fc}).any(axis=1)

# Filtrando o DataFrame para remover outliers inferiores
df_no_outliers = df_new[~outliers_combined]
'''

#Spliting dataset in training and testing
X = df_no_outliers.drop(['Griding time'], axis=1)
y = df_no_outliers['Griding time']

# Transformar o y em vetor numpy
y = np.array(y)

scaler = MinMaxScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

from re import sub
import matplotlib.pyplot as plt
import numpy as np

# Create the subplots
fig, axes = plt.subplots(4, 3, figsize=(15, 15))

axes[0,0].plot(X[:,1],y, 'o', color='chocolate')
axes[0,0].set_title('Initial Pressure vs Grinding Time')
axes[0,1].plot(X[:,2],y, 'o', color='chartreuse')
axes[0,1].set_title('Cut Off Flow Corrected vs Grinding Time')
axes[0,2].plot(X[:,4],y, 'o', color='aqua')
axes[0,2].set_title('Initial Flow Ratio Corrected vs Grinding Time')
axes[1,0].plot(X[:,5],y, 'o', color='red')
axes[1,0].set_title('Griding Temperature Oil vs Grinding Time')
axes[1,1].plot(X[:,12],y, 'o', color='darkviolet')
axes[1,1].set_title('Flow Rate Temperature Before Grinding vs Grinding Time')
axes[1,2].plot(X[:,7],y, 'o', color='black')
axes[1,2].set_title('Flow Pressure vs Grinding Time')
axes[2,0].plot(X[:,8],y, 'o', color='gold')
axes[2,0].set_title('Flow Corrected vs Grinding Time')
axes[2,1].plot(X[:,9],y, 'o', color='fuchsia')
axes[2,1].set_title('Oil Temperature vs Grinding Time')
axes[2,2].plot(X[:,11],y, 'o', color='lime')
axes[2,2].set_title('Flow Rate Before Grinding Corrected vs Grinding Time')
axes[3,0].plot(X[:,0],y, 'o', color='blue')
axes[3,0].set_title('Pressure 1 vs Grinding Time')
axes[3,1].plot(X[:,10],y, 'o', color='green')
axes[3,1].set_title('Flow Pressure Before Grinding vs Grinding Time')
axes[3,2].plot(X[:,6],y, 'o', color='orange')
axes[3,2].set_title('Rework Time vs Grinding Time')


plt.tight_layout()
plt.show

# Eliminar também: Initial Pressure, Oil temperature, Flow pressure before grinding.

'''
#excluding the outlier from flow press and florr corr columns from df_no_outliers
Q1 = df_no_outliers['Flow Pressure'].quantile(0.25)
Q3 = df_no_outliers['Flow Pressure'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.75 * IQR

# Removing 'any(axis=1)' as it's not needed for Series and causing the error
df_no_outliers = df_no_outliers[~(df_no_outliers['Flow Pressure'] < lower_bound)]

Q1 = df_no_outliers['Flow Corrected'].quantile(0.25)
Q3 = df_no_outliers['Flow Corrected'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.75 * IQR

# Removing 'any(axis=1)' as it's not needed for Series and causing the error
df_no_outliers = df_no_outliers[~(df_no_outliers['Flow Corrected'] < lower_bound)]
'''

# getting the maximun and minimum values of each columns of df_no_outliers
max_values = df_no_outliers.max()
min_values = df_no_outliers.min()

print("Máximos:")
print(max_values)

print("\nMínimos:")
print(min_values)

"""**Redes Neurais**
---


"""

#Excluding more features
df = df_no_outliers.copy()
df = df.drop(['Cut off flow rate', 'Initial Flow Rate', 'Flow Rate', 'Flow','flow rate before Grinding w/o back pressure','Pressure 1 Especification','Initial Pressure','Oil Temperature','Flow pressure before grinding w/o back pressure' ], axis=1)
print(df.info())
df.head()

#Spliting dataset in training and testing
X = df.drop(['Griding time'], axis=1)
y = df['Griding time']

# Transformar o y em vetor numpy
y = np.array(y)

scaler = MinMaxScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

from re import sub
import matplotlib.pyplot as plt
import numpy as np

# Create the subplots
fig, axes = plt.subplots(3, 3, figsize=(15, 15))

axes[0,0].plot(X[:,2],y, 'o', color='chocolate')
axes[0,0].set_title('Cut Off Flow - Especification vs Grinding Time')
axes[0,1].plot(X[:,1],y, 'o', color='chartreuse')
axes[0,1].set_title('Cut Off Flow Corrected vs Grinding Time')
axes[0,2].plot(X[:,3],y, 'o', color='aqua')
axes[0,2].set_title('Initial Flow Ratio Corrected vs Grinding Time')
axes[1,0].plot(X[:,4],y, 'o', color='red')
axes[1,0].set_title('Griding Temperature Oil vs Grinding Time')
axes[1,1].plot(X[:,9],y, 'o', color='darkviolet')
axes[1,1].set_title('Flow Rate Temperature Before Grinding vs Grinding Time')
axes[1,2].plot(X[:,6],y, 'o', color='black')
axes[1,2].set_title('Flow Pressure vs Grinding Time')
axes[2,0].plot(X[:,7],y, 'o', color='gold')
axes[2,0].set_title('Flow Corrected vs Grinding Time')
#axes[2,1].plot(X[:,9],y, 'o', color='fuchsia')
#axes[2,1].set_title('Oil Temperature vs Grinding Time')
axes[2,1].plot(X[:,8],y, 'o', color='lime')
axes[2,1].set_title('Flow Rate Before Grinding Corrected vs Grinding Time')
axes[2,2].plot(X[:,0],y, 'o', color='blue')
axes[2,2].set_title('Pressure 1 vs Grinding Time')
#axes[3,1].plot(X[:,10],y, 'o', color='green')
#axes[3,1].set_title('Flow Pressure Before Grinding vs Grinding Time')
#axes[3,2].plot(X[:,6],y, 'o', color='orange')
#axes[3,2].set_title('Rework Time vs Grinding Time')

plt.tight_layout()
plt.show

#Spliting dataset in training and testing
X = df.drop(['Griding time','Rework Time'], axis=1)
y = df[['Griding time','Rework Time']]

# Transformar o y em vetor numpy
y = np.array(y)

# Converting rework time in 0 or 1
for i in range(len(y)):
  if y[i,1]>0:
    y[i,1] = 1
  else:
    y[i,1] = 0

scaler = MinMaxScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)

y_test.shape

# Indentify proportion of zeros and ones in y_train
zero = 0
um = 0
for i in range(len(y_train[:,1])):
  if y_train[i,1]==0:
    zero = zero + 1
  else:
    um = um + 1
zero_percentil = (zero/len(y_train[:,1]))*100
um_percentil = (um/len(y_train[:,1]))*100
print(f'Percentual de zeros em treinamento: {zero_percentil}')
print(f'Percentual de uns em treinamento: {um_percentil}')

# Indentify proportion of zeros and ones in y_train
zero = 0
um = 0
for i in range(len(y_test[:,1])):
  if y_test[i,1]==0:
    zero = zero + 1
  else:
    um = um + 1
zero_percentil = (zero/len(y_test[:,1]))*100
um_percentil = (um/len(y_test[:,1]))*100
print(f'Percentual de zeros em teste: {zero_percentil}')
print(f'Percentual de uns em teste: {um_percentil}')

from tensorflow.keras.callbacks import ModelCheckpoint

checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import accuracy_score

# Define a entrada do modelo
inputs = keras.Input(shape=(X_train.shape[1],))

# Define as camadas compartilhadas
x = layers.Dense(64, activation='relu')(inputs)
x = layers.Dense(32, activation='relu')(x)
x = layers.Dense(16, activation='relu')(x)

# Define a saída de regressão
regression_output = layers.Dense(1, activation='linear', name='regression_output')(x)

# Define a saída de classificação
classification_output = layers.Dense(1, activation='sigmoid', name='classification_output')(x)

# Cria o modelo com duas saídas
model = keras.Model(inputs=inputs, outputs=[regression_output, classification_output])

# Define as métricas e funções de perda para cada saída
model.compile(
    optimizer='adam',
    loss={
        'regression_output': 'mse',
        'classification_output': 'binary_crossentropy'
    },
    loss_weights={
        'regression_output': 0.3,  # ou outro valor que você deseje
        'classification_output': 1.0
    },
    metrics={
        'regression_output': ['mae', 'mse'],
        'classification_output': ['accuracy', keras.metrics.AUC(name='AUC-PR', curve='PR')]
    }
)

# Treina o modelo com dois targets
history = model.fit(
    X_train, {'regression_output': y_train[:, 0], 'classification_output': y_train[:, 1]}, epochs=1000, validation_split=0.2, verbose=2, callbacks=[checkpoint])

# Avalia o modelo nos dados de teste
loss, regression_loss, classification_loss, regression_mae, regression_mse, classification_accuracy, classification_auc = model.evaluate(
    X_test,
    {'regression_output': y_test[:, 0], 'classification_output': y_test[:, 1]},
    verbose=2
)

from tensorflow.keras.models import load_model


metrics = model.evaluate(
    X_test,
    {'regression_output': y_test[:, 0], 'classification_output': y_test[:, 1]},
    verbose=2,
    return_dict=True  # Retorna um dicionário com as métricas
)


metrics_df = pd.DataFrame([metrics])

for column in metrics_df.columns:
    metrics_df[column] = metrics_df[column].round(decimals=2)

abreviacoes = {
    'loss': 'loss',
    'regression_output_loss': 'reg_loss',
    'val_regression_output_loss': 'val_reg_loss',
    'regression_output_mae': 'reg_mae',
    'val_regression_output_mae': 'val_reg_mae',
    'regression_output_mse': 'reg_mse',
    'val_regression_output_mse': 'val_reg_mse',
    'classification_output_loss': 'class_loss',
    'val_classification_output_loss': 'val_class_loss',
    'classification_output_accuracy': 'accu',
    'val_classification_output_accuracy': 'val_accu',
    'classification_output_AUC-PR': 'AUC-PR',
    'val_classification_output_auc': 'val_AUC'
}

metrics_df = metrics_df.rename(columns=abreviacoes)

fig, ax = plt.subplots()
ax.axis('off')
table = ax.table(cellText=metrics_df.values, colLabels=metrics_df.columns, loc='center')
table.auto_set_font_size(False)
table.set_fontsize(12)
table.scale(1.2, 1.2)
plt.show()

pesos = model.get_weights()
model.save_weights('pesos.weights.h5')

import matplotlib.pyplot as plt

# Plotagem da Loss de Regressão
plt.figure(figsize=(8, 6))  # Define o tamanho da figura
plt.plot(history.history['regression_output_loss'], label='Regression Training Loss', color='purple')
plt.plot(history.history['val_regression_output_loss'], label='Regression Validation Loss',color='orange')
plt.title('Regression Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

# Plotagem do MSE
plt.figure(figsize=(8, 6))
plt.plot(history.history['regression_output_mae'], label='Training MAE', color='purple')
plt.plot(history.history['val_regression_output_mae'], label='Validation MAE', color='orange')
plt.title('Mean Absolute Error')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

# Plotagem do MSE
plt.figure(figsize=(8, 6))
plt.plot(history.history['regression_output_mse'], label='Training MSE', color='purple')
plt.plot(history.history['val_regression_output_mse'], label='Validation MSE', color='orange')
plt.title('Loss - Mean Squared Error')
plt.ylabel('MSE')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

# Plotagem da Loss de Classificação
plt.figure(figsize=(8, 6))  # Define o tamanho da figura
plt.plot(history.history['classification_output_loss'], label='Classification Training Loss', color='purple')
plt.plot(history.history['val_classification_output_loss'], label='Classification Validation Loss', color='orange')
plt.title('Classification Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

# Plotagem da Accuracy
plt.figure(figsize=(8, 6))
plt.plot(history.history['classification_output_accuracy'], label='Training Accuracy', color='purple')
plt.plot(history.history['val_classification_output_accuracy'], label='Validation Accuracy', color='orange')
plt.title('Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='center right')
plt.show()

# Plotagem da Accuracy
plt.figure(figsize=(8, 6))
plt.plot(history.history['classification_output_AUC-PR'], label='Training AUC-PR', color='purple')
plt.plot(history.history['val_classification_output_AUC-PR'], label='Validation AUC-PR', color='orange')
plt.title('AUC-PR')
plt.ylabel('AUC-PR')
plt.xlabel('Epoch')
plt.legend(loc='center right')
plt.show()

from scipy.stats import pearsonr
from sklearn.metrics import mean_squared_error
from tensorflow.keras.metrics import MeanSquaredError

best_model = load_model('best_model.h5', custom_objects={'mse': MeanSquaredError})

y_pred = best_model.predict(X_test, verbose=2)
y_pred = np.asarray(y_pred)
y_pred = y_pred.squeeze() # Reduzir dimensão do vetor - 3D para 2D
y_pred = np.transpose(y_pred) # Transpor o vetor

r, p_value = pearsonr(y_test[:,0], y_pred[:,0])
print(f'Coeficiente de Correlação de Pearson (R): {r}')

from sklearn.metrics import r2_score

r2 = r2_score(y_test[:,0], y_pred[:,0])
print(f'R-quadrado: {r2}')

y_pred[1]

# Get the F1-score, accuracy, precision and recall
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

y_pred_binary = (y_pred[:,1] > 0.5).astype(int)

confusion_matrix = confusion_matrix(y_test[:,1],y_pred_binary)
accuracy = accuracy_score(y_test[:,1],y_pred_binary)
precision = precision_score(y_test[:,1],y_pred_binary)
recall = recall_score(y_test[:,1],y_pred_binary)
f1 = f1_score(y_test[:,1], y_pred_binary)


print(confusion_matrix)
print(f"Acurácia:{accuracy}")
print(f"Precisão:{precision}")
print(f"Recall:{recall}")
print(f"F1-score: {f1}")



def Target_vs_Predic(y_test,y_pred):
  plt.figure(figsize=(16, 6))
  plt.plot(y_test, 'o', color = 'darkslateblue', label='Target')
  plt.plot(y_pred, 'o', color = 'crimson', label='ANN prediction')
  plt.title('Target vs prediction of the ANN')
  plt.xlabel('Example')
  plt.legend()
  plt.show
  print(y_test)
  print(y_pred)

Target_vs_Predic(y_test[:,1],y_pred_binary)

# Create the plot
plt.figure(figsize=(10, 10))  # Adjust figure size if needed
plt.plot(y_test[:,0], y_pred[:,0], 'o', color='green', label='Matching')
#add ploting of a linear line
plt.plot([min(y_test[:,0]), max(y_test[:,0])], [min(y_test[:,0]), max(y_test[:,0])], color='purple', linestyle='--', label='Perfect Prediction')
plt.ylabel('Griding Time')
plt.title('Predicted vs. Actual Griding Time')
plt.legend()
#plt.grid(True)  # Add a grid for better visualization
plt.show()

erros_classificacao = np.where(y_test[:, 1] != (y_pred_binary).astype(int))[0]
for i in erros_classificacao:
  if y_test[i, 1] == 1:
    print(f"Caso {i}, Saída {y_pred[i,1]}")

"""**Radom Forest**
---


"""

from sklearn.ensemble import RandomForestRegressor

random_model = RandomForestRegressor(n_estimators=200, verbose=2, min_samples_split=2, max_depth = None, random_state=42)
random_model.fit(X_train, y_train)

y_pred = random_model.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

#Spliting dataset in training and testing
X = df.drop(['Griding time'], axis=1)
y = df['Griding time']

# Transformar o y em vetor numpy
y = np.array(y)

scaler = MinMaxScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Defina os hiperparâmetros a serem testados
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}

rf = RandomForestRegressor(random_state=42)

# Crie o objeto GridSearchCV
grid_search = GridSearchCV(rf, param_grid, cv=5, verbose=2)

# Treine o modelo com Grid Search
grid_search.fit(X_train, y_train)

# Imprima os melhores hiperparâmetros
print(grid_search.best_params_)

# Saving the best model
best_model = grid_search.best_estimator_

y_pred = best_model.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

import matplotlib.pyplot as plt
import numpy as np

# Create the plot
plt.figure(figsize=(10, 10))  # Adjust figure size if needed
plt.plot(y_pred, color='red', label='Predicted Values (y_pred)')
plt.plot(y_test, color='blue', label='Actual Values (y_test)')
plt.ylabel('Griding Time')
plt.title('Predicted vs. Actual Griding Time')
plt.legend()
#plt.grid(True)  # Add a grid for better visualization
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Create the plot
plt.figure(figsize=(10, 10))  # Adjust figure size if needed
plt.plot(y_test, y_pred, 'o', color='green', label='Matching')
#add ploting of a linear line
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='purple', linestyle='--', label='Perfect Prediction')
plt.ylabel('Griding Time')
plt.title('Predicted vs. Actual Griding Time')
plt.legend()
#plt.grid(True)  # Add a grid for better visualization
plt.show()

def add_gaussian_noise(data, noise_factor=0.05):
  noise = np.random.normal(0, data.std() * noise_factor, data.shape)
  return data + noise

X_test_noisy = add_gaussian_noise(X_test)

import numpy as np

def mixup_augmentation(X, y, alpha=0.2):
  num_samples = X.shape[0]
  indices = np.arange(num_samples)
  np.random.shuffle(indices)

  lambda_ = np.random.beta(alpha, alpha)

  X_mix = lambda_ * X + (1 - lambda_) * X[indices]
  y_mix = lambda_ * y + (1 - lambda_) * y[indices]

  return X_mix, y_mix

X_augmented, y_augmented = mixup_augmentation(X_test, y_test)

# Apllying gaussian noise in the test data
'''
noise = np.random.normal(0, X_test.std() * 0.05, X_test.shape)
X_test_noisy = X_test + noise
'''

y_pred_noisy = random_model.predict(X_test_noisy)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

plt.figure(figsize=(10, 10))
plt.plot(y_test,'o', color='paleturquoise', label='Actual Values')
plt.plot(y_pred_noisy,'o', color='green', label='Noisy Values')
plt.show()